{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89ebb61-e1e1-4fc7-978b-3eb6b3eafa6f",
   "metadata": {},
   "source": [
    "# TREE based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40056f9-3407-4ae4-90fe-502ac7c4d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09332a9f-48ed-4201-a8d7-ec59461a9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and X & Y saperation\n",
    "df = load_csv('../data/processed/Treated_subscriptions_churn.csv')\n",
    "X = df.drop([\"subscription_canceled\",\"is_active_last30d\",\"total_revenue\"],axis=1)\n",
    "y = df[\"subscription_canceled\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=16, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5d03b-d55b-494c-90c9-989ce43a777d",
   "metadata": {},
   "source": [
    "## RANDOM FOREST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7942117-6613-4301-8106-8ab4747fd2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Grid Search for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'max_features': ['sqrt','log2'],\n",
    "    'max_depth': [10,15,20],\n",
    "    'min_samples_split': [ 2,5,10],\n",
    "    'min_samples_leaf': [ 1,2,4],\n",
    "    'class_weight': ['balanced'] # Critical for churn prediction\n",
    "}\n",
    "\n",
    "print(\"Starting Random Forest Tuning...\")\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=16),\n",
    "    param_grid_rf,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Final Model and Evaluation ---\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "print(f\"--- Random Forest Results ---\")\n",
    "print(f\"\\nOptimal RF Parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Cross-Validated ROC AUC on Training Data: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on the unseen test data\n",
    "y_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "test_roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC AUC on Unseen Test Data: {test_roc_auc:.4f}\\n\")\n",
    "\n",
    "# Use a standard threshold (0.5) to get a classification report\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfe7ef-8019-417c-845b-20ecd0e02146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimal RF Parameters: {'class_weight': 'balanced', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 300}\n",
    "# # Cross-Validated ROC AUC on Training Data: 0.8374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6453f44-025f-4ca9-93cd-3f250452cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THRESHOLDS TUNING\n",
    "# rf_proba = best_rf.predict_proba(X_test)[:,1]\n",
    "# thresholds = np.arange(0.2, 0.8, 0.05)\n",
    "\n",
    "# for t in thresholds:\n",
    "#     rf_pred = (rf_proba >= t).astype(int)\n",
    "#     print(\"ROC AUC:\", roc_auc_score(y_test, rf_proba), \" @\", t)\n",
    "#     print(classification_report(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52451b21-314a-4c85-a478-cce5ac2fd369",
   "metadata": {},
   "source": [
    "### “Since churn is a cost-sensitive problem, we evaluated multiple probability thresholds between 0.20 and 0.80. While higher thresholds improved accuracy, they significantly reduced recall for churners. The F1-optimized thresholds range from 0.35 to 0.55, and the final choice is a business decision based on the cost function. We selected a threshold of 0.38, which maintains high churn recall (85%) while better precision and operational efficiency(73%). This provides the best balance between customer retention coverage and campaign cost.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab91da-def6-4d71-a7b7-c96504926ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Extract Feature Importance\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top 10 Random Forest Feature Importances ---\")\n",
    "print(importances.head(10).to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eee99e-37a5-4ce3-9c3a-74a8aebe8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Replace 'customer_id' with the EXACT name of the ID column in your processed CSV\n",
    "CUSTOMER_ID_COL = 'customer_id' \n",
    "# ---------------------\n",
    "\n",
    "# --- 1. Load the Original Processed Data (Contains IDs) ---\n",
    "df=pd.read_csv(\"../data/processed/cleaned_subscriptions_churn.csv\")\n",
    "print(f\"Loaded data with {df.shape[0]} rows.\")\n",
    "\n",
    "# Separate Target and Features\n",
    "# We use the target to ensure the split is identical to your modeling step\n",
    "y = df['subscription_canceled'].values.ravel()\n",
    "X = df.drop(columns=['subscription_canceled'])\n",
    "\n",
    "# --- 2. Re-Split to Isolate the Test Set IDs ---\n",
    "# We use the EXACT same parameters: test_size=0.2, random_state=42, stratify=y\n",
    "# We don't need to drop columns here because we only care about the indices\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=16, stratify=y)\n",
    "\n",
    "# --- 3. Generate Predictions ---\n",
    "# Use your trained 'best_rf' and the scaled test data 'XTest'\n",
    "# (These must be currently in your memory from previous steps)\n",
    "y_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply your optimized threshold\n",
    "CHURN_THRESHOLD = 0.38\n",
    "y_pred_final = (y_prob >= CHURN_THRESHOLD).astype(int)\n",
    "\n",
    "# --- 4. Assemble the Final Data Frame ---\n",
    "# Reset index to ensure 1:1 alignment between the Original Data rows and Predictions\n",
    "X_test_orig = X_test_orig.reset_index(drop=True)\n",
    "\n",
    "results_df = X_test_orig.copy()\n",
    "results_df['True_Churn_Status'] = y_test # Add true status\n",
    "results_df['Predicted_Probability'] = y_prob # Add model probability\n",
    "results_df['Predicted_Churn_Flag'] = y_pred_final # Add final decision\n",
    "\n",
    "# --- 5. Organize Columns (Put ID First) ---\n",
    "# Moves customer_id to the first column for readability\n",
    "if CUSTOMER_ID_COL in results_df.columns:\n",
    "    cols = [CUSTOMER_ID_COL] + [c for c in results_df.columns if c != CUSTOMER_ID_COL]\n",
    "    results_df = results_df[cols]\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT_DIR = \"../data/processed/tree_model\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Save Final Artifacts ---\n",
    "joblib.dump(best_rf,os.path.join(OUTPUT_DIR, \"best_rf_model.joblib\"))\n",
    "print(\"Saved Model:\", os.path.join(OUTPUT_DIR, \"best_rf_model.joblib\"))\n",
    "\n",
    "results_df.to_csv(os.path.join(OUTPUT_DIR, \"rf_churn_predictions_ACTIONABLE.csv\"),index=False)\n",
    "print(\"Saved Predictions:\", os.path.join(OUTPUT_DIR, \"rf_churn_predictions_ACTIONABLE.csv\"))\n",
    "\n",
    "\n",
    "# Print Summary\n",
    "print(f\"\\n--- Delivery Complete ---\")\n",
    "print(f\"Total Customers in Report: {len(results_df)}\")\n",
    "print(f\"Customers Flagged for Retention: {results_df['Predicted_Churn_Flag'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4928b0-326f-4ed8-93b2-1f032ea6116f",
   "metadata": {},
   "source": [
    "## XGBOOST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb6757-3679-49a1-be90-dc7b0b20cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 1. Calculate the imbalance ratio\n",
    "# This is crucial for ROC AUC in imbalanced datasets\n",
    "counts = y_train.value_counts()\n",
    "ratio = counts[0] / counts[1]\n",
    "\n",
    "# 2. Define the base model\n",
    "# We set n_estimators high because early_stopping will find the perfect cutoff\n",
    "xgb_model = XGBClassifier(\n",
    "    tree_method='hist',\n",
    "    eval_metric='auc',\n",
    "    scale_pos_weight=ratio, \n",
    "    early_stopping_rounds=50,\n",
    "    random_state=16\n",
    ")\n",
    "\n",
    "# 3. Define the Search Space\n",
    "# We focus on min_child_weight and gamma to control the imbalance sensitivity\n",
    "param_grid = {\n",
    "    'n_estimators': [2000],              # Early stopping handles the actual limit\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6],           # Keeping it shallow prevents overfitting imbalance\n",
    "    'min_child_weight': [1, 5, 10],      # Higher values prevent leaf nodes on outliers\n",
    "    'gamma': [0, 0.1, 0.2, 0.4],         # Minimum loss reduction for a split\n",
    "    'subsample': [0.7, 0.8, 0.9],        # Row sampling\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9], # Feature sampling\n",
    "    'reg_alpha': [0, 0.1, 1],            # L1 regularization\n",
    "    'reg_lambda': [1, 5, 10]             # L2 regularization\n",
    "}\n",
    "\n",
    "# 4. Execute Randomized Search\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50, \n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=16\n",
    ")\n",
    "\n",
    "# 5. Fit with Early Stopping\n",
    "# We pass eval_set to monitor performance on unseen data during training\n",
    "xgb_search.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "# 6. Final Evaluation\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "y_prob_xgb = best_xgb.predict_proba(X_test)[:, 1]\n",
    "final_auc = roc_auc_score(y_test, y_prob_xgb)\n",
    "\n",
    "print(f\"\\nOptimal XGB Parameters: {xgb_search.best_params_}\")\n",
    "print(f\"Cross-Validated ROC AUC on Training Data: {xgb_search.best_score_:.4f}\")\n",
    "\n",
    "print(f\"XGBoost Test ROC AUC: {final_auc:.4f}\")\n",
    "xgb_pred = (y_prob_xgb >= 0.5).astype(int)\n",
    "print(\"ROC AUC:\", final_auc)\n",
    "print(classification_report(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468766d-6dcc-449c-9095-5fab10b1ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # THRESHOLDS TUNING\n",
    "\n",
    "# y_prob_xgb = best_xgb.predict_proba(X_test)[:, 1]\n",
    "# thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "\n",
    "# for t in thresholds:\n",
    "#     xgb_pred = (y_prob_xgb >= t).astype(int)\n",
    "#     print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_xgb),\" @ \", t)\n",
    "#     print(classification_report(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de2e07-854b-47eb-b13f-d36533aad589",
   "metadata": {},
   "source": [
    "### “Since churn is a cost-sensitive problem, we evaluated multiple probability thresholds between 0.30 and 0.80. While higher thresholds improved accuracy, they significantly reduced recall for churners. The F1-optimized thresholds range from 0.45 to 0.65, and the final choice is a business decision based on the cost function. We selected a threshold of 0.55, which maintains high churn recall (88%) while improving precision(49%) and operational efficiency(72%). This provides the best balance between customer retention coverage and campaign cost.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1632ae-8ae7-44d7-a00d-c1fd932ff904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Extract Feature Importance\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_xgb.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top 10 XGBoost Feature Importances ---\")\n",
    "print(importances.head(10).to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976decd-3f15-496b-aee9-dc4b2ea64d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Replace 'customer_id' with the EXACT name of the ID column in your processed CSV\n",
    "CUSTOMER_ID_COL = 'customer_id' \n",
    "# ---------------------\n",
    "\n",
    "# --- 1. Load the Original Processed Data (Contains IDs) ---\n",
    "df=pd.read_csv(\"../data/processed/cleaned_subscriptions_churn.csv\")\n",
    "print(f\"Loaded data with {df.shape[0]} rows.\")\n",
    "\n",
    "# Separate Target and Features\n",
    "# We use the target to ensure the split is identical to your modeling step\n",
    "y = df['subscription_canceled'].values.ravel()\n",
    "X = df.drop(columns=['subscription_canceled'])\n",
    "\n",
    "# --- 2. Re-Split to Isolate the Test Set IDs ---\n",
    "# We use the EXACT same parameters: test_size=0.2, random_state=42, stratify=y\n",
    "# We don't need to drop columns here because we only care about the indices\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=16, stratify=y)\n",
    "\n",
    "# --- 3. Generate Predictions ---\n",
    "# Use your trained 'best_xgb' and the scaled test data 'XTest'\n",
    "# (These must be currently in your memory from previous steps)\n",
    "y_prob = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply your optimized threshold\n",
    "CHURN_THRESHOLD = 0.46\n",
    "y_pred_final = (y_prob >= CHURN_THRESHOLD).astype(int)\n",
    "\n",
    "# --- 4. Assemble the Final Data Frame ---\n",
    "# Reset index to ensure 1:1 alignment between the Original Data rows and Predictions\n",
    "X_test_orig = X_test_orig.reset_index(drop=True)\n",
    "\n",
    "results_df = X_test_orig.copy()\n",
    "results_df['True_Churn_Status'] = y_test # Add true status\n",
    "results_df['Predicted_Probability'] = y_prob # Add model probability\n",
    "results_df['Predicted_Churn_Flag'] = y_pred_final # Add final decision\n",
    "\n",
    "# --- 5. Organize Columns (Put ID First) ---\n",
    "# Moves customer_id to the first column for readability\n",
    "if CUSTOMER_ID_COL in results_df.columns:\n",
    "    cols = [CUSTOMER_ID_COL] + [c for c in results_df.columns if c != CUSTOMER_ID_COL]\n",
    "    results_df = results_df[cols]\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT_DIR = \"../data/processed/tree_model\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Save Final Artifacts ---\n",
    "joblib.dump(best_xgb,os.path.join(OUTPUT_DIR, \"best_xgb_model.joblib\"))\n",
    "print(\"Saved Model:\", os.path.join(OUTPUT_DIR, \"best_xgb_model.joblib\"))\n",
    "\n",
    "results_df.to_csv(os.path.join(OUTPUT_DIR, \"xgb_churn_predictions_ACTIONABLE.csv\"),index=False)\n",
    "print(\"Saved Predictions:\", os.path.join(OUTPUT_DIR, \"xgb_churn_predictions_ACTIONABLE.csv\"))\n",
    "\n",
    "\n",
    "# Print Summary\n",
    "print(f\"\\n--- Delivery Complete ---\")\n",
    "print(f\"Total Customers in Report: {len(results_df)}\")\n",
    "print(f\"Customers Flagged for Retention: {results_df['Predicted_Churn_Flag'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0ec26-f24c-4457-a88c-b48487357eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
