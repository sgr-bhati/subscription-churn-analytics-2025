{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33213691-e97f-4538-816a-924ae91eff80",
   "metadata": {},
   "source": [
    "## Model Comparison: L1 Logistic Regression vs Tree-Based Models (Random Forest & XGBoost)\n",
    "\n",
    "To identify the most suitable churn prediction model, three model families were evaluated: a linear baseline (L1 Logistic Regression) and two tree-based ensemble models (Random Forest and XGBoost). The comparison focuses on **predictive performance**, **churn recall**, **business usability**, and **model complexity**.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Criteria\n",
    "\n",
    "- **ROC AUC**: Overall ranking ability of the model\n",
    "- **Churn Recall (Class = 1)**: Ability to correctly identify churners (highest priority)\n",
    "- **Precision (Class = 1)**: Cost control for retention campaigns\n",
    "- **Accuracy**: Overall correctness (secondary metric)\n",
    "- **Interpretability**: Ease of explanation to business stakeholders\n",
    "- **Operational Readiness**: Scalability, tuning flexibility, and deployment feasibility\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Comparison (Test Set)\n",
    "\n",
    "| Model | Threshold | ROC AUC | Churn Recall | Churn Precision | Accuracy |\n",
    "|-----|----------|--------|--------------|-----------------|----------|\n",
    "| **L1 Logistic Regression** | 0.55 | ~0.76 | 81% | 53% | 76% |\n",
    "| **Random Forest (Base)** | 0.50 | ~0.85 | 74% | **58%** | 79% |\n",
    "| **Random Forest (Tuned)** | 0.38 | ~0.85 | 85% | 50% | 73% |\n",
    "| **XGBoost (Base)** | 0.50 | ~0.84 | 76% | 53% | 76% |\n",
    "| **XGBoost (Tuned)** | 0.46 | **~0.85** | **88%** | 49% | 72% |\n",
    "\n",
    "> **Note:** Threshold tuning significantly improved churn recall for tree-based models, which is critical for cost-sensitive churn use cases.\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths and Limitations by Model\n",
    "\n",
    "#### L1 Logistic Regression\n",
    "**Strengths**\n",
    "- Highly interpretable coefficients\n",
    "- Sparse feature selection via L1 regularization\n",
    "- Fast training and scoring\n",
    "\n",
    "**Limitations**\n",
    "- Limited ability to model non-linear relationships\n",
    "- Lower overall predictive power compared to ensembles\n",
    "\n",
    "**Best Use Case**\n",
    "- Explainability layer\n",
    "- Business insight generation\n",
    "- Baseline and sanity-check model\n",
    "\n",
    "---\n",
    "\n",
    "#### Random Forest\n",
    "**Strengths**\n",
    "- Handles non-linear interactions naturally\n",
    "- Stable performance with minimal overfitting\n",
    "- Clear feature importance rankings\n",
    "\n",
    "**Limitations**\n",
    "- Less flexible probability calibration\n",
    "- Slightly lower churn recall compared to XGBoost after tuning\n",
    "\n",
    "**Best Use Case**\n",
    "- Strong benchmark model\n",
    "- Secondary production model\n",
    "- Feature validation and robustness checks\n",
    "\n",
    "---\n",
    "\n",
    "#### XGBoost\n",
    "**Strengths**\n",
    "- Highest ROC AUC and churn recall\n",
    "- Excellent handling of complex feature interactions\n",
    "- Threshold tuning enables business-driven cost control\n",
    "- Early stopping improves generalization\n",
    "\n",
    "**Limitations**\n",
    "- Higher computational cost\n",
    "- Requires careful tuning and monitoring\n",
    "\n",
    "**Best Use Case**\n",
    "- Primary production model\n",
    "- Large-scale retention targeting\n",
    "- Cost-sensitive churn mitigation\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways from Comparison\n",
    "\n",
    "- Tree-based models significantly outperform linear models in capturing churn behavior.\n",
    "- Threshold tuning is more impactful than raw accuracy for churn use cases.\n",
    "- XGBoost delivers the best balance between **coverage (recall)** and **cost control (precision)**.\n",
    "- Logistic Regression remains essential for interpretability and trust.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Comparative Insight\n",
    "\n",
    "> **For churn prediction, the model with the highest recall at an acceptable precision is more valuable than the most accurate model.**\n",
    "\n",
    "Based on this principle:\n",
    "- **XGBoost** is selected as the primary model.\n",
    "- **Random Forest** acts as a strong benchmark and backup.\n",
    "- **L1 Logistic Regression** supports explainability and stakeholder communication.\n",
    "\n",
    "This layered approach ensures both **business alignment** and **technical excellence** in the capstone solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b78a58-acc5-4d79-aad6-00538b972bcc",
   "metadata": {},
   "source": [
    "# Final Model Selection Rationale\n",
    "\n",
    "The primary business objective of this project is **churn prevention**, a cost-sensitive problem where **failing to identify a churner is significantly more expensive than incorrectly targeting a non-churner**. Therefore, model selection prioritizes **high churn recall** while maintaining acceptable precision and operational efficiency.\n",
    "\n",
    "### Models Evaluated\n",
    "- **L1 Logistic Regression**\n",
    "- **Random Forest**\n",
    "- **XGBoost**\n",
    "\n",
    "Each model was evaluated using ROC AUC, churn-class recall, precision, accuracy, and interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "### Why XGBoost Was Selected as the Primary Model\n",
    "\n",
    "**XGBoost** is selected as the final production model due to the following reasons:\n",
    "\n",
    "- **Highest Predictive Power**\n",
    "  - Achieved the best ROC AUC (~0.85) on unseen test data, indicating superior ranking ability.\n",
    "- **Maximum Churn Coverage**\n",
    "  - Delivered the highest churn recall (≈88%) after threshold tuning, ensuring most at-risk customers are identified.\n",
    "- **Robust to Complex Patterns**\n",
    "  - Effectively captures non-linear interactions among behavioral, usage, and billing features.\n",
    "- **Business-Controlled Trade-off**\n",
    "  - Probability threshold tuning (final threshold = 0.46) allows direct alignment with marketing budget and campaign cost constraints.\n",
    "- **Generalization Control**\n",
    "  - Use of early stopping during training reduces overfitting and improves real-world robustness.\n",
    "\n",
    "As churn mitigation is coverage-driven, XGBoost provides the best balance between **risk minimization and operational effectiveness**.\n",
    "\n",
    "---\n",
    "\n",
    "### Role of Random Forest in the Project\n",
    "\n",
    "**Random Forest** demonstrated performance close to XGBoost and offers:\n",
    "\n",
    "- Strong churn recall (≈85%) with stable accuracy\n",
    "- Easier interpretability via feature importance rankings\n",
    "- Lower computational complexity compared to XGBoost\n",
    "\n",
    "While not selected as the primary model, Random Forest serves as:\n",
    "- A **strong benchmark**\n",
    "- A **fallback model** in case of operational constraints\n",
    "- A validation reference for feature-level insights\n",
    "\n",
    "---\n",
    "\n",
    "### Why L1 Logistic Regression Was Retained\n",
    "\n",
    "**L1 Logistic Regression** was not selected as the primary predictor but remains critical for:\n",
    "\n",
    "- **Explainability**\n",
    "  - Sparse coefficients clearly identify churn and retention drivers.\n",
    "- **Business Communication**\n",
    "  - Easier interpretation for non-technical stakeholders.\n",
    "- **Model Sanity Checks**\n",
    "  - Ensures consistency of major drivers across linear and non-linear models.\n",
    "\n",
    "This model complements XGBoost by providing transparency and trust.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Decision Summary\n",
    "\n",
    "| Aspect | Selected Approach |\n",
    "|------|------------------|\n",
    "| Primary Scoring Model | **XGBoost** |\n",
    "| Threshold Strategy | Tuned probability threshold (0.46) |\n",
    "| Explainability Layer | L1 Logistic Regression |\n",
    "| Backup / Benchmark | Random Forest |\n",
    "| Business Focus | Maximize churn recall with controlled precision |\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "A **dual-model strategy** is adopted:\n",
    "- **XGBoost** for high-performance churn prediction and retention targeting\n",
    "- **L1 Logistic Regression** for interpretability and stakeholder confidence\n",
    "\n",
    "This approach ensures the solution is both **technically robust** and **business-aligned**, making it suitable for real-world churn mitigation scenarios and a strong data science capstone deliverable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7d3ab-3504-4b59-b9f2-c5d0671edd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
